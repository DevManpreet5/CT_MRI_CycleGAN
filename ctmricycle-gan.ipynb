{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2222626,"sourceType":"datasetVersion","datasetId":1322457}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cycle-GAN for CT To MRI Image Translation\n\n## Overview\nThis project uses the Cycle-GAN Generative Adversarial Network (GAN) to translate CT images into MRI scans. The model is trained on paired MRI and CT images, learning to generate realistic MRI scans from CT inputs. This approach can be useful in medical imaging by reducing the need for multiple scans and enhancing diagnostic workflows.\n\n## Model Architecture\nCycle-GAN consists of two primary components:\n\n- **Generator:** A U-Net-based architecture that takes an MRI image as input and generates a corresponding CT image.\n- **Discriminator:** A PatchGAN-based model that determines whether an image is a real CT scan or a generated one.\n\nThe generator learns to fool the discriminator, while the discriminator improves its ability to distinguish real from fake images, leading to high-quality image translations.\n\n## Dataset\nThe dataset consists of paired MRI and CT images:\n\n- `trainA`: CT images\n- `trainB`: Corresponding MRI images\n\nDuring training, the model takes an CT image from `trainA` and attempts to generate a realistic MRI image that matches the corresponding image from `trainB`.\n\n## Training Details\n\n### Loss Functions\n- **Adversarial Loss (BCE Loss):** Encourages the generator to produce realistic MRI images.\n- **L1 Loss:** Ensures structural similarity between the generated and real MRI images.\n\n### Optimization\n- Adam optimizer with a learning rate of `0.0002` and betas `(0.5, 0.999)`.\n\n### Data Augmentation\n- Images are resized to `256x256` and normalized.\n\n\n## Usage\nTrained models are saved as:\n\n- `generator.pth`\n- `discriminator.pth`\n\n## Dependencies\nEnsure the following dependencies are installed:\n\n```bash\npip install torch torchvision pillow numpy scikit-image matplotlib\n```\n\n## Conclusion\nThis Cycle-GAN-based model effectively converts CT images into MRI scans, reducing the need for multiple imaging modalities and enhancing medical imaging workflows. Further improvements can be made by training on larger datasets or incorporating attention mechanisms for better detail preservation.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\nimport numpy as np\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass cycleganDataset(Dataset):\n    def __init__(self, trainA_path, trainB_path, transform=None):\n        self.trainA_images = sorted([os.path.join(trainA_path, img) for img in os.listdir(trainA_path)])\n        self.trainB_images = sorted([os.path.join(trainB_path, img) for img in os.listdir(trainB_path)])\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.trainA_images), len(self.trainB_images))\n\n    def __getitem__(self, idx):\n        img_A = Image.open(self.trainA_images[idx]).convert('RGB')\n        img_B = Image.open(self.trainB_images[idx]).convert('RGB')\n        if self.transform:\n            img_A = self.transform(img_A)\n            img_B = self.transform(img_B)\n        return img_A, img_B","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, 4, 2, 1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(6, 64, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, 4, 1, 1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(512, 1, 4, 1, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x, y):\n        x = torch.cat([x, y], dim=1)\n        return self.model(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_generated_images(real_A, real_B, fake_B, epoch, save_dir):\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    real_A = real_A.cpu().numpy()\n    real_B = real_B.cpu().numpy()\n    fake_B = fake_B.cpu().numpy()\n\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    plt.title('CT Scan')\n    plt.imshow(np.transpose(real_A[0], (1, 2, 0)) * 0.5 + 0.5)\n    plt.subplot(1, 3, 2)\n    plt.title('Ground Truth')\n    plt.imshow(np.transpose(real_B[0], (1, 2, 0)) * 0.5 + 0.5)\n    plt.subplot(1, 3, 3)\n    plt.title('Generated Image')\n    plt.imshow(np.transpose(fake_B[0], (1, 2, 0)) * 0.5 + 0.5)\n    plt.savefig(os.path.join(save_dir, f'epoch_{epoch}.png'))\n    plt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ndataset = cycleganDataset('/kaggle/input/ct-to-mri-cgan/Dataset/images/trainA', '/kaggle/input/ct-to-mri-cgan/Dataset/images/trainB', transform)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\ngenerator.apply(weights_init)\ndiscriminator.apply(weights_init)\n\ncriterion = nn.BCELoss()\nl1_loss = nn.L1Loss()\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\nsave_dir = '/kaggle/working/images'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\nfor epoch in range(1):\n    for i, (real_A, real_B) in enumerate(dataloader):\n        real_A, real_B = real_A.to(device), real_B.to(device)\n        fake_B = generator(real_A)\n\n        optimizer_D.zero_grad()\n        real_labels = torch.ones(real_A.size(0), 1, 30, 30).to(device)\n        fake_labels = torch.zeros(real_A.size(0), 1, 30, 30).to(device)\n        real_loss = criterion(discriminator(real_B, real_A), real_labels)\n        fake_loss = criterion(discriminator(fake_B.detach(), real_A), fake_labels)\n        d_loss = (real_loss + fake_loss) / 2\n        d_loss.backward()\n        optimizer_D.step()\n\n        optimizer_G.zero_grad()\n        g_loss = criterion(discriminator(fake_B, real_A), real_labels)\n        l1 = l1_loss(fake_B, real_B)\n        g_total_loss = g_loss + 100 * l1\n        g_total_loss.backward()\n        optimizer_G.step()\n\n        if i % 100 == 0:\n            print(f'Epoch [{epoch}/{500}], Step [{i}/{len(dataloader)}], '\n                  f'D_loss: {d_loss.item():.4f}, G_loss: {g_total_loss.item():.4f}')\n\n    if epoch % 100 == 0:\n        with torch.no_grad():\n            fake_B = generator(real_A)\n            fake_B = fake_B.detach().cpu()\n            real_A = real_A.cpu()\n            real_B = real_B.cpu()\n\n            save_generated_images(real_A, real_B, fake_B, epoch, save_dir)\n\ntorch.save(generator.state_dict(), 'generator.pth')\ntorch.save(discriminator.state_dict(), 'discriminator.pth')\nprint(\"Models saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}